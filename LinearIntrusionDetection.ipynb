{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeff-Rudolph/anomaly-based-intrusion-detection/blob/main/LinearIntrusionDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzCVOaZtj9sD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas\n",
        "import sklearn.ensemble\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.metrics as tfm\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWrgghhgtDuV"
      },
      "outputs": [],
      "source": [
        "file_size = int(input(\"Would you like to use 10% data file or full file?(0 - %, 1 - Full):\"))\n",
        "if(file_size == 0):\n",
        "  pdata = pandas.read_csv('/content/drive/My Drive/data/kddcup10pct.txt',header=None)\n",
        "if(file_size == 1):\n",
        "  pdata = pandas.read_csv('/content/drive/My Drive/data/kddcup_data_corrected.txt',header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oadVLaxlGNvB"
      },
      "outputs": [],
      "source": [
        "feature_names_str = '''duration: continuous.\n",
        "protocol_type: symbolic.\n",
        "service: symbolic.\n",
        "flag: symbolic.\n",
        "src_bytes: continuous.\n",
        "dst_bytes: continuous.\n",
        "land: symbolic.\n",
        "wrong_fragment: continuous.\n",
        "urgent: continuous.\n",
        "hot: continuous.\n",
        "num_failed_logins: continuous.\n",
        "logged_in: symbolic.\n",
        "num_compromised: continuous.\n",
        "root_shell: continuous.\n",
        "su_attempted: continuous.\n",
        "num_root: continuous.\n",
        "num_file_creations: continuous.\n",
        "num_shells: continuous.\n",
        "num_access_files: continuous.\n",
        "num_outbound_cmds: continuous.\n",
        "is_host_login: symbolic.\n",
        "is_guest_login: symbolic.\n",
        "count: continuous.\n",
        "srv_count: continuous.\n",
        "serror_rate: continuous.\n",
        "srv_serror_rate: continuous.\n",
        "rerror_rate: continuous.\n",
        "srv_rerror_rate: continuous.\n",
        "same_srv_rate: continuous.\n",
        "diff_srv_rate: continuous.\n",
        "srv_diff_host_rate: continuous.\n",
        "dst_host_count: continuous.\n",
        "dst_host_srv_count: continuous.\n",
        "dst_host_same_srv_rate: continuous.\n",
        "dst_host_diff_srv_rate: continuous.\n",
        "dst_host_same_src_port_rate: continuous.\n",
        "dst_host_srv_diff_host_rate: continuous.\n",
        "dst_host_serror_rate: continuous.\n",
        "dst_host_srv_serror_rate: continuous.\n",
        "dst_host_rerror_rate: continuous.\n",
        "dst_host_srv_rerror_rate: continuous.'''\n",
        "\n",
        "temp = feature_names_str.split('.')\n",
        "feature_names = []\n",
        "\n",
        "\n",
        "for x in temp:\n",
        "  x = x.replace(\"\\n\",\"\")\n",
        "  x = x.replace(\": continuous\",\"\")\n",
        "  x = x.replace(\": symbolic\",\"\")\n",
        "  feature_names.append(x)\n",
        "print(len(feature_names))\n",
        "\n",
        "feature_names.pop() #removes unnecessary empty string element at end of list\n",
        "feature_names.append('event') \n",
        "\n",
        "pdata.columns = feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XwIR1mk0rrg"
      },
      "outputs": [],
      "source": [
        "#inspecting initial dataset\n",
        "print(pdata.shape)\n",
        "print(pdata['event'].value_counts()/len(pdata)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO1crW84JEwV"
      },
      "outputs": [],
      "source": [
        "pdata.drop_duplicates(keep='first', inplace = True) \n",
        "#removes duplicates if inplace=true \n",
        "#arg subset=false(default) this way things will only be removed if \n",
        "#100% match including event"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN3U9FXKJIou"
      },
      "outputs": [],
      "source": [
        "print(pdata.shape)#checking new size after removing dupes\n",
        "print(pdata['event'].value_counts()/len(pdata)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdzzGk7OT_iP"
      },
      "outputs": [],
      "source": [
        "#pdata.var(axis=0,numeric_only=True) \n",
        "#num outbound commands variance is 0 can drop this col\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0mGcfzCzAnC"
      },
      "outputs": [],
      "source": [
        "protocol_list = pandas.factorize(pdata['protocol_type'])[0]\n",
        "pdata['protocol_type_dense'] = protocol_list\n",
        "pdata['protocol_type_dense'] = pdata['protocol_type_dense'] - 1\n",
        "\n",
        "\n",
        "service_list = pandas.factorize(pdata['service'])[0]\n",
        "pdata['service_type_dense'] = service_list\n",
        "if(file_size == 0):\n",
        "  pdata['service_type_dense'] = pdata['service_type_dense'] - 33\n",
        "if(file_size == 1):\n",
        "  pdata['service_type_dense'] = pdata['service_type_dense'] - 35\n",
        "\n",
        "flag_list = pandas.factorize(pdata['flag'])[0]\n",
        "pdata['flag_type_dense'] = flag_list\n",
        "pdata['flag_type_dense'] = pdata['flag_type_dense'] - 5\n",
        "\n",
        "#NOTE: sparse features and these dense dummy encodings are both being calculated\n",
        "#the model will run on the sparse OH encoded features called wide_inputs later \n",
        "#on. You can try and run the model with deep_inputs (dummies) however the \n",
        "#performance is very poor. Both of these input types are used in wide & deep model\n",
        "#found in this GitHub project.\n",
        "\n",
        "\n",
        "#dummy variable text to int conversion for these categorical inputs to avoid\n",
        "#curse of dimensionality problems caused by OH encode need to make sure these \n",
        "#dont get normalized.\n",
        "#subtractions are to balance numbers around 0 \n",
        "#(10% and full have diff numbers for service tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nue3MD9g0C_p"
      },
      "outputs": [],
      "source": [
        "x_data = pdata.drop(['event'], axis=1)\n",
        "\n",
        "y_data = pdata['event'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq4Xzuiy6K4k"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for i in range(len(y_data)):\n",
        "  if(y_data[i]=='normal.'):\n",
        "    count = count + 1\n",
        "balance = len(y_data)-count\n",
        "print(\"Number of normals:\",count)\n",
        "print(\"Number of data points:\",len(y_data))\n",
        "print(\"percent normal:\",count/(len(y_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiR8oGcbxM_O"
      },
      "outputs": [],
      "source": [
        "classification = int(input(\"Would you like to run with binary or multiclass Y values?(0 - binary, 1 - multi):\"))\n",
        "if(classification == 0):\n",
        "  for i in range(len(y_data)):\n",
        "    if y_data[i] in ['normal.']:\n",
        "      y_data[i] = 'normal'\n",
        "    else:\n",
        "      y_data[i] = 'hack' #converting to binary problem\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF4Qfxib0CPg"
      },
      "outputs": [],
      "source": [
        "x_data = x_data.drop(['num_outbound_cmds'],axis=1)\n",
        "#dropped for 0 variance in entire file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBluSPFWa0hZ"
      },
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test = sklearn.model_selection.train_test_split(x_data,y_data,test_size=0.3) \n",
        "#randomly shuffle and then split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4IUPVHL9UwD"
      },
      "outputs": [],
      "source": [
        "x_normalizer = StandardScaler()\n",
        "\n",
        "x_train_norm = x_train\n",
        "x_test_norm = x_test\n",
        "\n",
        "continous_list = ['duration', 'src_bytes', 'dst_bytes', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'num_compromised',\n",
        "                  'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'count',\n",
        "                  'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
        "                  'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "                  'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
        "                  'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
        "\n",
        "#only apply Z score (StandardScaler) to numerical continous cols\n",
        "\n",
        "x_train_norm[continous_list] = x_normalizer.fit_transform(x_train[continous_list])\n",
        "x_test_norm[continous_list] = x_normalizer.transform(x_test[continous_list])\n",
        "\n",
        "y_one_hot_encoder = sklearn.preprocessing.OneHotEncoder(sparse=False,handle_unknown='ignore')\n",
        "y_test = y_test.reshape(-1,1)\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_train = y_one_hot_encoder.fit_transform(y_train)\n",
        "y_test = y_one_hot_encoder.transform(y_test)#one hot encode Y\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wide_input = list(x_train_norm.columns)\n",
        "#print(wide_input)"
      ],
      "metadata": {
        "id": "huOv5PVDnfRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "protocol_cat = list(x_train['protocol_type'].values) #pull entire col into list\n",
        "protocol_cat = list(set(protocol_cat)) #turn list into mathematical set to remove duplicates\n",
        "prot_OH_encoder = CountVectorizer(vocabulary=protocol_cat, binary=True) #only 0 or 1 in sparse matrix\n",
        "protocol_train = prot_OH_encoder.fit_transform(x_train['protocol_type'].values)\n",
        "protocol_test = prot_OH_encoder.transform(x_test['protocol_type'].values)\n",
        "\n",
        "\n",
        "\n",
        "service_cat = list(x_train['service'].values) #do same for service\n",
        "service_cat = list(set(service_cat))\n",
        "serv_OH_encoder = CountVectorizer(vocabulary=service_cat, binary=True,lowercase=False)\n",
        "service_train = serv_OH_encoder.fit_transform(x_train['service'].values)\n",
        "service_test = serv_OH_encoder.transform(x_test['service'].values)\n",
        "\n",
        "\n",
        "flag_cat = list(x_train['flag'].values)#same for flag\n",
        "flag_cat = list(set(flag_cat))\n",
        "flag_OH_encoder = CountVectorizer(vocabulary=flag_cat, binary=True,lowercase=False)\n",
        "flag_train = flag_OH_encoder.fit_transform(x_train['flag'].values)\n",
        "flag_test = flag_OH_encoder.transform(x_test['flag'].values)\n",
        "\n",
        "print(flag_train[3])\n",
        "print(flag_train[3].toarray())\n",
        "  \n"
      ],
      "metadata": {
        "id": "VrAFzW4xityp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "protocol_train_cols = protocol_train.toarray()\n",
        "service_train_cols = service_train.toarray() \n",
        "#turn the csr's into regular matrix(2D list)\n",
        "flag_train_cols = flag_train.toarray()\n",
        "\n",
        "protocol_test_cols = protocol_test.toarray()\n",
        "service_test_cols = service_test.toarray()\n",
        "flag_test_cols = flag_test.toarray()"
      ],
      "metadata": {
        "id": "X8vRdRhuqAv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#turning the OH encoded features into matrices to be joined to the original DF's \n",
        "protocol_colnames = prot_OH_encoder.get_feature_names()\n",
        "train_prot_df = pandas.DataFrame(protocol_train_cols, columns=protocol_colnames) \n",
        "test_prot_df = pandas.DataFrame(protocol_test_cols, columns=protocol_colnames)\n",
        "\n",
        "service_colnames = serv_OH_encoder.get_feature_names()\n",
        "train_serv_df = pandas.DataFrame(service_train_cols, columns=service_colnames)\n",
        "test_serv_df = pandas.DataFrame(service_test_cols, columns=service_colnames)\n",
        "\n",
        "flag_colnames = flag_OH_encoder.get_feature_names()\n",
        "train_flag_df = pandas.DataFrame(flag_train_cols, columns=flag_colnames)\n",
        "test_flag_df = pandas.DataFrame(flag_test_cols, columns=flag_colnames)\n",
        "\n"
      ],
      "metadata": {
        "id": "VWZZ8RSbo798"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_OH_features = pandas.concat([train_prot_df,train_serv_df,train_flag_df],axis=1)\n",
        "test_OH_features = pandas.concat([test_prot_df,test_serv_df,test_flag_df],axis=1)"
      ],
      "metadata": {
        "id": "2pENnzmWwHAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#have to match indexes OH matrix doesnt have the shuffled indexes from x matrix\n",
        "train_OH_features.index = x_train_norm.index\n",
        "test_OH_features.index = x_test_norm.index\n",
        "\n",
        "x_train_norm = pandas.concat([x_train_norm,train_OH_features], axis=1)\n",
        "x_test_norm = pandas.concat([x_test_norm,test_OH_features], axis=1)"
      ],
      "metadata": {
        "id": "A4AweGgBqs74"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deep_inputs = list(train_OH_features.columns.values) + continous_list"
      ],
      "metadata": {
        "id": "j_HVahYTITcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "deep_symbolic_features = list(set(feature_names) - set(continous_list) -set(['num_outbound_cmds','event','flag','service','protocol_type']))\n",
        "wide_symbolic_features = list(set(deep_symbolic_features))\n",
        "deep_symbolic_features = deep_symbolic_features + ['protocol_type_dense',\t'service_type_dense',\t'flag_type_dense']\n",
        "\n",
        "\n",
        "deep_inputs = continous_list + deep_symbolic_features\n",
        "wide_inputs = list(train_OH_features.columns.values) + continous_list + wide_symbolic_features\n",
        "# print(len(list(x_train_norm[wide_inputs].columns)))\n",
        "# print(len(list(x_train_norm[deep_inputs].columns)))"
      ],
      "metadata": {
        "id": "3WMyqgA2IsGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(classification == 0):\n",
        "  out_size = 2\n",
        "  linear_func = 'sigmoid'#binary\n",
        "else:\n",
        "  out_size = len(y_train[0])\n",
        "  linear_func = 'softmax'#multiclass\n",
        "\n",
        "wide_model = keras.experimental.LinearModel(activation=linear_func,units=out_size, use_bias=False)\n",
        "\n",
        "if(classification==0):\n",
        "  wide_model.compile(\n",
        "      loss=keras.losses.BinaryCrossentropy(from_logits=False), \n",
        "      optimizer=keras.optimizers.Adam(), \n",
        "      metrics=[ \"accuracy\"],\n",
        "  )\n",
        "else:\n",
        "  wide_model.compile(\n",
        "      loss=keras.losses.CategoricalCrossentropy(from_logits=False), \n",
        "      optimizer=keras.optimizers.Adam(), \n",
        "      metrics=[ \"accuracy\"],\n",
        "  )"
      ],
      "metadata": {
        "id": "BSbb1C3MNDfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        # Stop training when `val_loss` is no longer improving\n",
        "        monitor=\"loss\",\n",
        "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
        "        min_delta=1e-2,\n",
        "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
        "        patience=2,\n",
        "        verbose=1,\n",
        "    ),\n",
        "]\n",
        "\n",
        "wide_model.fit(x_train_norm[wide_inputs],y_train,batch_size=64, epochs=100, callbacks=callbacks)"
      ],
      "metadata": {
        "id": "7nVI6yE_TSK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdDvpFbW92o1"
      },
      "outputs": [],
      "source": [
        "y_pred = wide_model.predict(x_test_norm[wide_inputs])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities = (y_pred)\n",
        "predictions = []\n",
        "for z in range(len(probabilities)):\n",
        "  probabilities[z] = [float(round(x)) for x in probabilities[z]]\n",
        "\n",
        "for i in range(len(probabilities)):\n",
        "  for j in range(len(probabilities[0])):\n",
        "    if(probabilities[i][j] > 1):\n",
        "      probabilities[i][j] = 1\n",
        "\n",
        "label = y_one_hot_encoder.get_feature_names_out()\n",
        "norm_pos = 1\n",
        "for i in range(len(label)):\n",
        "  if label[i] == 'x0_normal.':\n",
        "    norm_pos = i\n",
        "#print(norm_pos)\n",
        "\n",
        "for i in range(len(label)):\n",
        "  label[i] = i\n",
        "label = np.delete(label,norm_pos)\n",
        "\n",
        "#due to the way this dataset was constructed the Recall the only valid metric:\n",
        "#“Trivial detection using the TTL aside, we found that it was still useful to \n",
        "# evaluate the true positive performance of a network IDS; however, any false \n",
        "# positive results were meaningless” (Brugger, 2007)\n",
        "\n",
        "print(sklearn.metrics.recall_score(y_test,probabilities,average='macro', labels=label))\n",
        "#the above line takes a simple average of the recall score for each class except\n",
        "#the normal class. The formula for recall is TP/TP+FN to avoid div by 0 this \n",
        "#method has built in measures. If all cases are properly identified then it \n",
        "#becomes TP/TP+0 = 1, if the event is not in the sample then recall is set to 0\n",
        "#The Data's imbalance will cause 0's to appear and drive down the macro average\n",
        "\n",
        "# print(sklearn.metrics.accuracy_score(y_test,probabilities,normalize=False)) \n",
        "# raw number of correct predictions\n",
        "# print(len(y_test))\n",
        "# total number of events "
      ],
      "metadata": {
        "id": "DlKys5YUQSiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_values = sklearn.metrics.recall_score(y_test,probabilities,average=None,zero_division=0)\n",
        "test_keys = y_one_hot_encoder.get_feature_names_out()\n",
        "res = {test_keys[i]: test_values[i] for i in range(len(test_keys))}\n",
        "#aligning up recall results in a dictionary just for personal visualizing\n",
        "print(res)"
      ],
      "metadata": {
        "id": "-Qx-0NmFVDAf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "LinearIntrusionDetection.ipynb",
      "provenance": [],
      "mount_file_id": "19-qSdUSiduwpXUVITVGLd5nL1itiBZ9d",
      "authorship_tag": "ABX9TyNhzdmJ+FS7Sl5DPeOXvI6/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}