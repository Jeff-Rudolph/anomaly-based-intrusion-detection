{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeff-Rudolph/anomaly-based-intrusion-detection/blob/main/WideDeepIntrusionDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzCVOaZtj9sD"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import scipy\n",
        "import sklearn\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import sklearn.ensemble as ske\n",
        "import pandas\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.sparse import hstack\n",
        "from sklearn import metrics\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.metrics as tfm\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWrgghhgtDuV"
      },
      "outputs": [],
      "source": [
        "file_size = int(input(\"Would you like to use 10% data file or full file?(0 - %, 1 - Full):\"))\n",
        "if(file_size == 0):\n",
        "  pdata = pandas.read_csv('/content/drive/My Drive/data/kddcup10pct.txt',header=None)\n",
        "if(file_size == 1):\n",
        "  pdata = pandas.read_csv('/content/drive/My Drive/data/kddcup_data_corrected.txt',header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oadVLaxlGNvB"
      },
      "outputs": [],
      "source": [
        "feature_names_str = '''duration: continuous.\n",
        "protocol_type: symbolic.\n",
        "service: symbolic.\n",
        "flag: symbolic.\n",
        "src_bytes: continuous.\n",
        "dst_bytes: continuous.\n",
        "land: symbolic.\n",
        "wrong_fragment: continuous.\n",
        "urgent: continuous.\n",
        "hot: continuous.\n",
        "num_failed_logins: continuous.\n",
        "logged_in: symbolic.\n",
        "num_compromised: continuous.\n",
        "root_shell: continuous.\n",
        "su_attempted: continuous.\n",
        "num_root: continuous.\n",
        "num_file_creations: continuous.\n",
        "num_shells: continuous.\n",
        "num_access_files: continuous.\n",
        "num_outbound_cmds: continuous.\n",
        "is_host_login: symbolic.\n",
        "is_guest_login: symbolic.\n",
        "count: continuous.\n",
        "srv_count: continuous.\n",
        "serror_rate: continuous.\n",
        "srv_serror_rate: continuous.\n",
        "rerror_rate: continuous.\n",
        "srv_rerror_rate: continuous.\n",
        "same_srv_rate: continuous.\n",
        "diff_srv_rate: continuous.\n",
        "srv_diff_host_rate: continuous.\n",
        "dst_host_count: continuous.\n",
        "dst_host_srv_count: continuous.\n",
        "dst_host_same_srv_rate: continuous.\n",
        "dst_host_diff_srv_rate: continuous.\n",
        "dst_host_same_src_port_rate: continuous.\n",
        "dst_host_srv_diff_host_rate: continuous.\n",
        "dst_host_serror_rate: continuous.\n",
        "dst_host_srv_serror_rate: continuous.\n",
        "dst_host_rerror_rate: continuous.\n",
        "dst_host_srv_rerror_rate: continuous.'''\n",
        "\n",
        "temp = feature_names_str.split('.')\n",
        "feature_names = []\n",
        "\n",
        "\n",
        "for x in temp:\n",
        "  x = x.replace(\"\\n\",\"\")\n",
        "  x = x.replace(\": continuous\",\"\")\n",
        "  x = x.replace(\": symbolic\",\"\")\n",
        "  feature_names.append(x)\n",
        "#print(len(feature_names))\n",
        "\n",
        "feature_names.pop() #removes unnecessary empty string element at end of list\n",
        "feature_names.append('event') \n",
        "\n",
        "pdata.columns = feature_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XwIR1mk0rrg"
      },
      "outputs": [],
      "source": [
        "#inspecting initial dataset\n",
        "print(pdata.shape)\n",
        "print(pdata['event'].value_counts()/len(pdata)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO1crW84JEwV"
      },
      "outputs": [],
      "source": [
        "pdata.drop_duplicates(keep='first', inplace = True) \n",
        "#removes duplicates if inplace=true \n",
        "#arg subset=false(default) this way things will only be removed if \n",
        "#100% match including event"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LN3U9FXKJIou"
      },
      "outputs": [],
      "source": [
        "print(pdata.shape)#checking new size after removing dupes\n",
        "print(pdata['event'].value_counts()/len(pdata)*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdzzGk7OT_iP"
      },
      "outputs": [],
      "source": [
        "#pdata.var(axis=0,numeric_only=True) \n",
        "#num outbound commands variance is 0 can drop this col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0mGcfzCzAnC"
      },
      "outputs": [],
      "source": [
        "protocol_list = pandas.factorize(pdata['protocol_type'])[0]\n",
        "pdata['protocol_type_dense'] = protocol_list\n",
        "pdata['protocol_type_dense'] = pdata['protocol_type_dense'] - 1\n",
        "\n",
        "\n",
        "service_list = pandas.factorize(pdata['service'])[0]\n",
        "pdata['service_type_dense'] = service_list\n",
        "if(file_size == 0):\n",
        "  pdata['service_type_dense'] = pdata['service_type_dense'] - 33\n",
        "if(file_size == 1):\n",
        "  pdata['service_type_dense'] = pdata['service_type_dense'] - 35\n",
        "\n",
        "flag_list = pandas.factorize(pdata['flag'])[0]\n",
        "pdata['flag_type_dense'] = flag_list\n",
        "pdata['flag_type_dense'] = pdata['flag_type_dense'] - 5\n",
        "\n",
        "\n",
        "#dummy variable text to int conversion for these categorical inputs to avoid\n",
        "#curse of dimensionality problems caused by OH encode need to make sure these \n",
        "#dont get normalized.\n",
        "#subtractions are to balance numbers around 0 \n",
        "#(10% and full have diff numbers for service tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nue3MD9g0C_p"
      },
      "outputs": [],
      "source": [
        "x_data = pdata.drop(['event'], axis=1)\n",
        "\n",
        "y_data = pdata['event'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vq4Xzuiy6K4k"
      },
      "outputs": [],
      "source": [
        "count = 0\n",
        "for i in range(len(y_data)):\n",
        "  if(y_data[i]=='normal.'):\n",
        "    count = count + 1\n",
        "balance = len(y_data)-count\n",
        "print(\"Number of normals:\",count)\n",
        "print(\"Number of data points:\",len(y_data))\n",
        "print(\"percent normal:\",count/(len(y_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiR8oGcbxM_O"
      },
      "outputs": [],
      "source": [
        "classification = int(input(\"Would you like to run with binary or multiclass Y values?(0 - binary, 1 - multi):\"))\n",
        "if(classification == 0):\n",
        "  for i in range(len(y_data)):\n",
        "    if y_data[i] in ['normal.']:\n",
        "      y_data[i] = 'normal'\n",
        "    else:\n",
        "      y_data[i] = 'hack'\n",
        "  #turning into binary problem\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bF4Qfxib0CPg"
      },
      "outputs": [],
      "source": [
        "x_data = x_data.drop(['num_outbound_cmds'],axis=1)\n",
        "#dropped for 0 variance in entire file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBluSPFWa0hZ"
      },
      "outputs": [],
      "source": [
        "x_train,x_test,y_train,y_test = sklearn.model_selection.train_test_split(x_data,y_data,test_size=0.3) \n",
        "#random shuffle then split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4IUPVHL9UwD"
      },
      "outputs": [],
      "source": [
        "x_normalizer = StandardScaler()\n",
        "\n",
        "x_train_norm = x_train\n",
        "x_test_norm = x_test\n",
        "\n",
        "continous_list = ['duration', 'src_bytes', 'dst_bytes', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'num_compromised',\n",
        "                  'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files', 'count',\n",
        "                  'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
        "                  'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "                  'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
        "                  'dst_host_rerror_rate', 'dst_host_srv_rerror_rate']\n",
        "\n",
        "\n",
        "x_train_norm[continous_list] = x_normalizer.fit_transform(x_train[continous_list])\n",
        "x_test_norm[continous_list] = x_normalizer.transform(x_test[continous_list])\n",
        "\n",
        "y_one_hot_encoder = sklearn.preprocessing.OneHotEncoder(sparse=False,handle_unknown='ignore')\n",
        "y_test = y_test.reshape(-1,1)\n",
        "y_train = y_train.reshape(-1,1)\n",
        "y_train = y_one_hot_encoder.fit_transform(y_train)\n",
        "y_test = y_one_hot_encoder.transform(y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrAFzW4xityp"
      },
      "outputs": [],
      "source": [
        "protocol_cat = list(x_train['protocol_type'].values) #pull entire col into list\n",
        "protocol_cat = list(set(protocol_cat)) #turn list into mathematical set to remove duplicates\n",
        "\n",
        "\n",
        "prot_OH_encoder = CountVectorizer(vocabulary=protocol_cat, binary=True) #only 0 or 1 in sparse matrix\n",
        "protocol_train = prot_OH_encoder.fit_transform(x_train['protocol_type'].values)\n",
        "\n",
        "protocol_test = prot_OH_encoder.transform(x_test['protocol_type'].values)\n",
        "\n",
        "\n",
        "service_cat = list(x_train['service'].values) #do same for service\n",
        "service_cat = list(set(service_cat))\n",
        "serv_OH_encoder = CountVectorizer(vocabulary=service_cat, binary=True,lowercase=False)\n",
        "service_train = serv_OH_encoder.fit_transform(x_train['service'].values)\n",
        "service_test = serv_OH_encoder.transform(x_test['service'].values)\n",
        "\n",
        "\n",
        "flag_cat = list(x_train['flag'].values)#same for flag\n",
        "flag_cat = list(set(flag_cat))\n",
        "flag_OH_encoder = CountVectorizer(vocabulary=flag_cat, binary=True,lowercase=False)\n",
        "flag_train = flag_OH_encoder.fit_transform(x_train['flag'].values)\n",
        "flag_test = flag_OH_encoder.transform(x_test['flag'].values)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8vRdRhuqAv9"
      },
      "outputs": [],
      "source": [
        "protocol_train_cols = protocol_train.toarray()\n",
        "service_train_cols = service_train.toarray() #turn the csr's into regular matrix(2D list)\n",
        "flag_train_cols = flag_train.toarray()\n",
        "\n",
        "protocol_test_cols = protocol_test.toarray()\n",
        "service_test_cols = service_test.toarray()\n",
        "flag_test_cols = flag_test.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWZZ8RSbo798"
      },
      "outputs": [],
      "source": [
        "#turning the OH encoded features into matrices to be joined to the original DF's \n",
        "protocol_colnames = prot_OH_encoder.get_feature_names()\n",
        "train_prot_df = pandas.DataFrame(protocol_train_cols, columns=protocol_colnames) \n",
        "test_prot_df = pandas.DataFrame(protocol_test_cols, columns=protocol_colnames)\n",
        "\n",
        "service_colnames = serv_OH_encoder.get_feature_names()\n",
        "train_serv_df = pandas.DataFrame(service_train_cols, columns=service_colnames)\n",
        "test_serv_df = pandas.DataFrame(service_test_cols, columns=service_colnames)\n",
        "\n",
        "flag_colnames = flag_OH_encoder.get_feature_names()\n",
        "train_flag_df = pandas.DataFrame(flag_train_cols, columns=flag_colnames)\n",
        "test_flag_df = pandas.DataFrame(flag_test_cols, columns=flag_colnames)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pENnzmWwHAf"
      },
      "outputs": [],
      "source": [
        "train_OH_features = pandas.concat([train_prot_df,train_serv_df,train_flag_df],axis=1)\n",
        "test_OH_features = pandas.concat([test_prot_df,test_serv_df,test_flag_df],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4AweGgBqs74"
      },
      "outputs": [],
      "source": [
        "#have to match indexes OH matrix doesnt have the shuffled indexes from x matrix\n",
        "train_OH_features.index = x_train_norm.index\n",
        "test_OH_features.index = x_test_norm.index\n",
        "\n",
        "x_train_norm = pandas.concat([x_train_norm,train_OH_features], axis=1)\n",
        "x_test_norm = pandas.concat([x_test_norm,test_OH_features], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_HVahYTITcn"
      },
      "outputs": [],
      "source": [
        "wide_inputs = list(train_OH_features.columns.values) + continous_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WMyqgA2IsGi"
      },
      "outputs": [],
      "source": [
        "deep_symbolic_features = list(set(feature_names) - set(continous_list) -set(['num_outbound_cmds','event','flag','service','protocol_type']))\n",
        "wide_symbolic_features = list(set(deep_symbolic_features))\n",
        "deep_symbolic_features = deep_symbolic_features + ['protocol_type_dense',\t'service_type_dense',\t'flag_type_dense']\n",
        "\n",
        "\n",
        "deep_inputs = continous_list + deep_symbolic_features\n",
        "wide_inputs = list(train_OH_features.columns.values) + continous_list + wide_symbolic_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSbb1C3MNDfQ"
      },
      "outputs": [],
      "source": [
        "if(classification == 0):\n",
        "  out_size = 2\n",
        "  linear_func = 'sigmoid'\n",
        "else:\n",
        "  out_size = len(y_train[0])\n",
        "  linear_func = 'softmax'\n",
        "\n",
        "wide_model = keras.experimental.LinearModel(activation=linear_func,units=out_size, use_bias=False)\n",
        "\n",
        "if(classification==0):\n",
        "  wide_model.compile(\n",
        "      loss=keras.losses.BinaryCrossentropy(from_logits=False), \n",
        "      optimizer=keras.optimizers.Adam(), \n",
        "      metrics=[ tf.keras.metrics.Recall(class_id=1),tf.keras.metrics.Recall(class_id=0) ],\n",
        "  )\n",
        "else:\n",
        "  wide_model.compile(\n",
        "      loss=keras.losses.CategoricalCrossentropy(from_logits=False), \n",
        "      optimizer=keras.optimizers.Adam(), \n",
        "      metrics=[ tf.keras.metrics.Recall(class_id=i) for i in range(len(y_train[0]))],\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nVI6yE_TSK7"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        # Stop training when `val_loss` is no longer improving\n",
        "        monitor=\"loss\",\n",
        "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
        "        min_delta=1e-2,\n",
        "        # \"no longer improving\" being further defined as \"for at least 3 epochs\"\n",
        "        patience=3,\n",
        "        verbose=1,\n",
        "    ),\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuRnqoG0TI4P"
      },
      "outputs": [],
      "source": [
        "###########################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO5LdOF8X22e"
      },
      "outputs": [],
      "source": [
        "if classification == 0 :\n",
        "  last_hidden = 10\n",
        "  last_func = 'sigmoid'\n",
        "else:\n",
        "  last_hidden = 32\n",
        "  last_func = 'softmax'\n",
        "\n",
        "dnn_model = keras.Sequential()\n",
        "dnn_model.add(tf.keras.layers.Dense(50, input_dim=x_train_norm[deep_inputs].shape[1], activation='relu')) #Funneling down into the size of the output layer with ballpark appropriate sizes of hiddens\n",
        "dnn_model.add(tf.keras.layers.Dense(last_hidden, activation='relu')) \n",
        "dnn_model.add(tf.keras.layers.Dense(out_size, activation=last_func)) \n",
        "#makes sure the output layer is the size of the OH matrix generated by y_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwNhj7-LZ2T_"
      },
      "outputs": [],
      "source": [
        "if(classification==0):\n",
        "  dnn_model.compile(\n",
        "      loss=keras.losses.BinaryCrossentropy(from_logits=False), \n",
        "      optimizer=keras.optimizers.Adam(), \n",
        "      metrics=[tf.keras.metrics.Recall(class_id=1),tf.keras.metrics.Recall(class_id=0) ],\n",
        "  )\n",
        "else:\n",
        "  dnn_model.compile(\n",
        "      loss=keras.losses.CategoricalCrossentropy(from_logits=False), \n",
        "      optimizer=keras.optimizers.Adam(), \n",
        "      metrics=[ tf.keras.metrics.Recall(class_id=i) for i in range(len(y_train[0]))],\n",
        "  )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPshuPDlppgM"
      },
      "outputs": [],
      "source": [
        "combined_model = keras.experimental.WideDeepModel(wide_model, dnn_model, activation='relu')\n",
        "\n",
        "if(classification==0):\n",
        "  combined_model.compile(\n",
        "      loss=keras.losses.BinaryCrossentropy(from_logits=False), \n",
        "      optimizer=keras.optimizers.Adam(), \n",
        "      metrics=[ \"accuracy\" ],\n",
        "  )\n",
        "else:\n",
        "  combined_model.compile(\n",
        "      loss=keras.losses.CategoricalCrossentropy(from_logits=False), \n",
        "      optimizer=keras.optimizers.Adam(), \n",
        "      metrics=[ \"accuracy\"],\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ig7rVBHcjmqO"
      },
      "outputs": [],
      "source": [
        "combined_model.fit([x_train_norm[wide_inputs],x_train_norm[deep_inputs]], y_train, batch_size=64, epochs=100, callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io3J4MjXnMr_"
      },
      "outputs": [],
      "source": [
        "y_pred = combined_model.predict([x_test_norm[wide_inputs],x_test_norm[deep_inputs]])\n",
        "#predict Y values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAT7671DWRBf"
      },
      "outputs": [],
      "source": [
        "probabilities = (y_pred)\n",
        "predictions = []\n",
        "for z in range(len(probabilities)):\n",
        "  probabilities[z] = [float(round(x)) for x in probabilities[z]]\n",
        "\n",
        "for i in range(len(probabilities)):\n",
        "  for j in range(len(probabilities[0])):\n",
        "    if(probabilities[i][j] > 1):\n",
        "      probabilities[i][j] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFeXUsUafRJQ"
      },
      "outputs": [],
      "source": [
        "label = y_one_hot_encoder.get_feature_names_out()\n",
        "norm_pos = 1\n",
        "for i in range(len(label)):\n",
        "  if label[i] == 'x0_normal.':\n",
        "    norm_pos = i\n",
        "#print(norm_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo4oKKlCy7mQ"
      },
      "outputs": [],
      "source": [
        "for i in range(len(label)):\n",
        "  label[i] = i\n",
        "label = np.delete(label,norm_pos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x5RhizEoou6"
      },
      "outputs": [],
      "source": [
        "#due to the way this dataset was constructed the Recall the only valid metric:\n",
        "#“Trivial detection using the TTL aside, we found that it was still useful to \n",
        "# evaluate the true positive performance of a network IDS; however, any false \n",
        "# positive results were meaningless” (Brugger, 2007)\n",
        "\n",
        "print(sklearn.metrics.recall_score(y_test,probabilities,average='macro', labels=label))\n",
        "#the above line takes a simple average of the recall score for each class except\n",
        "#the normal class. The formula for recall is TP/TP+FN to avoid div by 0 this \n",
        "#method has built in measures. If all cases are properly identified then it \n",
        "#becomes TP/TP+0 = 1, if the event is not in the sample then recall is set to 0\n",
        "#The Data's imbalance will cause 0's to appear and drive down the macro average\n",
        "\n",
        "#print(sklearn.metrics.accuracy_score(y_test,probabilities,normalize=False)) \n",
        "#raw number of correct predictions\n",
        "#print(len(y_test))\n",
        "#total number of events "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZccdT2y4o0hI"
      },
      "outputs": [],
      "source": [
        "test_values = sklearn.metrics.recall_score(y_test,probabilities,average=None,zero_division=0)\n",
        "test_keys = y_one_hot_encoder.get_feature_names_out()\n",
        "res = {test_keys[i]: test_values[i] for i in range(len(test_keys))}\n",
        "#aligning up recall results in a dictionary just for personal visualizing\n",
        "print(res)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "WideDeepIntrusionDetection.ipynb",
      "provenance": [],
      "mount_file_id": "19-qSdUSiduwpXUVITVGLd5nL1itiBZ9d",
      "authorship_tag": "ABX9TyNhNfleFky+LCkiU54Fky6w",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}